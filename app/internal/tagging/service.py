import requests
from sqlalchemy.orm import Session
from typing import Tuple, List, Optional, Dict
from app.database.models import Sentence, Token

TAGGER_URL = "http://80.72.180.130:8040/api/tagging"

FEATURES_DICTIONARY = {
    "NOUN": ["Case", "Number", "Poss"],
    "PRON": ["Case", "Number", "PronType"],
    "PROPN": ["Case", "Number"],
    "ADJ": ["Degree", "AdjForm"],
    "NUM": ["NumType"],
    "VERB": ["Tense", "Person", "Number", "Mood", "Polarity", "Voice", "VerbForm"],
    "ADV": ["AdvForm"],
    "ATOOCH": ["Case", "Number", "Poss"],
    "KTOOCH": ["Case", "Number", "Poss"]
}


class TaggingService:
    def __init__(self, db: Session):
        self.db = db

    def _parse_conllu(self, conllu_text: str) -> List[Tuple[str, List[Dict]]]:
        """
        –ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ conllu. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π: (sentence_text, tokens)
        token: { token_index, form, lemma, pos, xpos, feats(dict|None) }
        """
        result = []
        current_sentence_text: Optional[str] = None
        current_tokens: List[Dict] = []

        lines = conllu_text.splitlines()
        current_index = 1
        for line in lines:
            line = line.strip()
            if not line:
                # –∫–æ–Ω–µ—Ü –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –µ—Å–ª–∏ –±—ã–ª–∏ —Ç–æ–∫–µ–Ω—ã ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º
                if current_sentence_text is not None:
                    result.append((current_sentence_text, current_tokens))
                current_sentence_text = None
                current_tokens = []
                current_index = 1
                continue

            if line.startswith('#'):
                if line.startswith('# text = '):
                    current_sentence_text = line[len('# text = '):]
                # –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ (# newdoc, # newpar, # sent_id, ...)
                continue

            # —Å—Ç—Ä–æ–∫–∏ —Ç–æ–∫–µ–Ω–æ–≤: id, form, lemma, upos, xpos, feats, ... —Ä–∞–∑–¥–µ–ª–µ–Ω—ã —Ç–∞–±—É–ª—è—Ü–∏–µ–π
            parts = line.split('\t')
            if len(parts) < 5:
                continue
            # parts[0] ‚Äî –º–æ–∂–µ—Ç –±—ã—Ç—å —á–∏—Å–ª–æ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ—Å—Ç–∞–≤–Ω—ã–µ –≤—Ä–æ–¥–µ 1-2
            if '-' in parts[0]:
                continue
            try:
                int(parts[0])
            except ValueError:
                continue

            form = parts[1]
            lemma = parts[2]
            pos = parts[3]
            xpos = parts[4]
            feats_raw = parts[5] if len(parts) > 5 else '_'

            feats: Optional[Dict[str, str]] = None
            if feats_raw and feats_raw != '_' and '=' in feats_raw:
                # feats —Ñ–æ—Ä–º–∞—Ç–∞ A=B|C=D
                feats = {}
                for item in feats_raw.split('|'):
                    if '=' in item:
                        k, v = item.split('=', 1)
                        feats[k] = v
            
            # üîπ –í–∞–ª–∏–¥–∞—Ü–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π
            feats = self._validate_feats(pos, xpos, feats)

            current_tokens.append({
                'token_index': str(current_index),
                'form': form,
                'lemma': lemma,
                'pos': pos,
                'xpos': xpos,
                'feats': feats,
            })
            current_index += 1

        # –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –ø—É—Å—Ç–æ–π —Å—Ç—Ä–æ–∫–æ–π, –¥–æ–±–∏–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        if current_sentence_text is not None:
            result.append((current_sentence_text, current_tokens))

        return result

    def tag_and_store(self, text: str) -> Tuple[int, int]:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —Ç–µ–≥–≥–µ—Ä, –ø–∞—Ä—Å–∏—Ç conllu –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –ë–î."""
        response = requests.post(TAGGER_URL, json={'text': text}, timeout=120)
        response.raise_for_status()
        data = response.json()
        conllu = data.get('conllu', '')

        parsed = self._parse_conllu(conllu)
        sentences_created = 0
        tokens_created = 0

        for sentence_text, tokens in parsed:
            sentence = Sentence(text=sentence_text, is_corrected=0)
            self.db.add(sentence)
            self.db.flush()  # –ø–æ–ª—É—á–∏—Ç—å sentence.id
            sentences_created += 1

            for token in tokens:
                db_token = Token(
                    token_index=token['token_index'],
                    form=token['form'],
                    lemma=token['lemma'],
                    pos=token['pos'],
                    xpos=token['xpos'],
                    feats=token['feats'],
                    sentence_id=sentence.id,
                )
                self.db.add(db_token)
                tokens_created += 1

        self.db.commit()
        return sentences_created, tokens_created


    def _validate_feats(self, pos: str, xpos: str, feats: Optional[Dict[str, str]]) -> Optional[Dict[str, str]]:
        """–§–∏–ª—å—Ç—Ä—É–µ—Ç feats –Ω–∞ –æ—Å–Ω–æ–≤–µ FEATURES_DICTIONARY, –ø—Ä–∏–≤–æ–¥—è –≤—Å—ë –∫ –≤–µ—Ä—Ö–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É."""
        if not feats:
            return None

        pos_upper = pos.upper() if pos else ""
        xpos_upper = xpos.upper() if xpos else ""

        allowed_features = FEATURES_DICTIONARY.get(pos_upper)
        if not allowed_features:
            allowed_features = FEATURES_DICTIONARY.get(xpos_upper)

        if allowed_features:
            feats = {k: v for k, v in feats.items() if k.capitalize() in allowed_features}
            if not feats:
                return None
            return feats
        else:
            # –ï—Å–ª–∏ UPOS –∏ XPOS –Ω–µ—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ ‚Üí –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –±—ã—Ç—å –Ω–µ –¥–æ–ª–∂–Ω–æ
            return None